{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fabc4c4",
   "metadata": {},
   "source": [
    "# Mental Health Data Cleaning and Preprocessing\n",
    "\n",
    "This notebook handles:\n",
    "- Data cleaning and validation\n",
    "- Missing value treatment\n",
    "- Outlier detection and handling\n",
    "- Feature engineering\n",
    "- Data standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195a894",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ea6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our preprocessing module\n",
    "from data.preprocessing import DataPreprocessor\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807cc5fb",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor(data_dir=\"../data\")\n",
    "\n",
    "# Load raw data\n",
    "try:\n",
    "    raw_data = preprocessor.load_raw_data()\n",
    "    print(f\"‚úì Raw data loaded successfully: {raw_data.shape}\")\n",
    "    print(f\"  Rows: {raw_data.shape[0]:,}\")\n",
    "    print(f\"  Columns: {raw_data.shape[1]}\")\n",
    "    \n",
    "    print(\"\\nColumn names:\")\n",
    "    for i, col in enumerate(raw_data.columns, 1):\n",
    "        print(f\"{i:2d}. {col}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    raw_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d78af",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e046bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_data is not None:\n",
    "    print(\"=== BEFORE CLEANING ===\")\n",
    "    print(f\"Shape: {raw_data.shape}\")\n",
    "    print(f\"Missing values: {raw_data.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Apply cleaning\n",
    "    cleaned_data = preprocessor.clean_mental_health_data(raw_data)\n",
    "    \n",
    "    print(\"\\n=== AFTER CLEANING ===\")\n",
    "    print(f\"Shape: {cleaned_data.shape}\")\n",
    "    print(f\"Missing values: {cleaned_data.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Show changes\n",
    "    rows_removed = raw_data.shape[0] - cleaned_data.shape[0]\n",
    "    print(f\"\\nüìä Data cleaning summary:\")\n",
    "    print(f\"  ‚Ä¢ Rows removed: {rows_removed:,} ({rows_removed/raw_data.shape[0]*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Columns standardized: {len(cleaned_data.columns)}\")\n",
    "    \n",
    "    display(cleaned_data.head())\n",
    "else:\n",
    "    print(\"‚ùå No raw data available for cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ff88b",
   "metadata": {},
   "source": [
    "## 4. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cleaned_data' in locals():\n",
    "    print(\"=== MISSING VALUES ANALYSIS ===\")\n",
    "    \n",
    "    missing_info = cleaned_data.isnull().sum()\n",
    "    missing_pct = (missing_info / len(cleaned_data)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_info,\n",
    "        'Missing Percentage': missing_pct\n",
    "    }).sort_values('Missing Count', ascending=False)\n",
    "    \n",
    "    print(\"Missing values by column:\")\n",
    "    display(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    # Visualize missing patterns\n",
    "    if missing_df['Missing Count'].sum() > 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Missing values bar chart\n",
    "        missing_cols = missing_df[missing_df['Missing Count'] > 0]\n",
    "        if len(missing_cols) > 0:\n",
    "            axes[0].barh(range(len(missing_cols)), missing_cols['Missing Percentage'])\n",
    "            axes[0].set_yticks(range(len(missing_cols)))\n",
    "            axes[0].set_yticklabels(missing_cols.index)\n",
    "            axes[0].set_xlabel('Missing Percentage (%)')\n",
    "            axes[0].set_title('Missing Data by Column')\n",
    "        \n",
    "        # Missing pattern heatmap\n",
    "        sample_size = min(1000, len(cleaned_data))\n",
    "        sample_data = cleaned_data.sample(sample_size)\n",
    "        \n",
    "        missing_matrix = sample_data.isnull().astype(int)\n",
    "        if missing_matrix.sum().sum() > 0:\n",
    "            sns.heatmap(missing_matrix.T, cbar=True, ax=axes[1], \n",
    "                       cmap='viridis', yticklabels=True)\n",
    "            axes[1].set_title(f'Missing Data Pattern (Sample of {sample_size} rows)')\n",
    "            axes[1].set_xlabel('Records')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚úÖ No missing values found after cleaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d960d46",
   "metadata": {},
   "source": [
    "## 5. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13956143",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cleaned_data' in locals():\n",
    "    print(\"=== OUTLIER DETECTION ===\")\n",
    "    \n",
    "    # Find prevalence columns\n",
    "    prevalence_cols = [col for col in cleaned_data.columns if 'prevalence' in col]\n",
    "    \n",
    "    if prevalence_cols:\n",
    "        fig, axes = plt.subplots(2, len(prevalence_cols), figsize=(4*len(prevalence_cols), 10))\n",
    "        \n",
    "        if len(prevalence_cols) == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        outlier_summary = {}\n",
    "        \n",
    "        for i, col in enumerate(prevalence_cols):\n",
    "            data_series = cleaned_data[col].dropna()\n",
    "            \n",
    "            if len(data_series) > 0:\n",
    "                # Box plot\n",
    "                axes[0, i].boxplot(data_series)\n",
    "                axes[0, i].set_title(f'{col.replace(\"_\", \" \").title()}\\nBox Plot')\n",
    "                axes[0, i].set_ylabel('Prevalence (%)')\n",
    "                \n",
    "                # Histogram\n",
    "                axes[1, i].hist(data_series, bins=30, alpha=0.7, edgecolor='black')\n",
    "                axes[1, i].set_title(f'{col.replace(\"_\", \" \").title()}\\nDistribution')\n",
    "                axes[1, i].set_xlabel('Prevalence (%)')\n",
    "                axes[1, i].set_ylabel('Frequency')\n",
    "                \n",
    "                # Calculate outliers using IQR method\n",
    "                Q1 = data_series.quantile(0.25)\n",
    "                Q3 = data_series.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers = data_series[(data_series < lower_bound) | (data_series > upper_bound)]\n",
    "                outlier_summary[col] = {\n",
    "                    'count': len(outliers),\n",
    "                    'percentage': len(outliers) / len(data_series) * 100,\n",
    "                    'lower_bound': lower_bound,\n",
    "                    'upper_bound': upper_bound\n",
    "                }\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print outlier summary\n",
    "        print(\"\\nOutlier Detection Summary (IQR method):\")\n",
    "        for metric, info in outlier_summary.items():\n",
    "            print(f\"\\n{metric}:\")\n",
    "            print(f\"  ‚Ä¢ Outliers found: {info['count']} ({info['percentage']:.1f}%)\")\n",
    "            print(f\"  ‚Ä¢ Valid range: {info['lower_bound']:.2f} - {info['upper_bound']:.2f}\")\n",
    "    else:\n",
    "        print(\"No prevalence columns found for outlier detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0925c7c",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cleaned_data' in locals():\n",
    "    print(\"=== FEATURE ENGINEERING ===\")\n",
    "    \n",
    "    # Add derived features\n",
    "    enhanced_data = preprocessor.add_derived_features(cleaned_data)\n",
    "    \n",
    "    new_features = set(enhanced_data.columns) - set(cleaned_data.columns)\n",
    "    \n",
    "    print(f\"Original columns: {len(cleaned_data.columns)}\")\n",
    "    print(f\"Enhanced columns: {len(enhanced_data.columns)}\")\n",
    "    print(f\"New features added: {len(new_features)}\")\n",
    "    \n",
    "    print(\"\\nNew features:\")\n",
    "    for i, feature in enumerate(sorted(new_features), 1):\n",
    "        print(f\"{i:2d}. {feature}\")\n",
    "    \n",
    "    # Show sample of enhanced data\n",
    "    print(\"\\nSample of enhanced data:\")\n",
    "    display(enhanced_data.head())\n",
    "    \n",
    "    # Analyze new features\n",
    "    if 'region' in enhanced_data.columns:\n",
    "        print(\"\\nRegional distribution:\")\n",
    "        region_counts = enhanced_data['region'].value_counts()\n",
    "        for region, count in region_counts.items():\n",
    "            print(f\"  {region}: {count} records\")\n",
    "        \n",
    "        # Visualize regional distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        region_counts.plot(kind='bar')\n",
    "        plt.title('Data Distribution by Region')\n",
    "        plt.xlabel('Region')\n",
    "        plt.ylabel('Number of Records')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37acb36c",
   "metadata": {},
   "source": [
    "## 7. Load and Merge External Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690552ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'enhanced_data' in locals():\n",
    "    print(\"=== LOADING EXTERNAL DATA ===\")\n",
    "    \n",
    "    # Load external datasets\n",
    "    external_data = preprocessor.load_external_data()\n",
    "    \n",
    "    print(f\"External datasets found: {len(external_data)}\")\n",
    "    for name, df in external_data.items():\n",
    "        print(f\"  ‚Ä¢ {name}: {df.shape}\")\n",
    "    \n",
    "    if external_data:\n",
    "        # Merge with main dataset\n",
    "        merged_data = preprocessor.merge_datasets(enhanced_data, external_data)\n",
    "        \n",
    "        print(f\"\\nData before merge: {enhanced_data.shape}\")\n",
    "        print(f\"Data after merge: {merged_data.shape}\")\n",
    "        \n",
    "        new_cols = set(merged_data.columns) - set(enhanced_data.columns)\n",
    "        if new_cols:\n",
    "            print(f\"\\nNew columns from external data:\")\n",
    "            for col in sorted(new_cols):\n",
    "                print(f\"  ‚Ä¢ {col}\")\n",
    "        \n",
    "        # Show sample of merged data\n",
    "        print(\"\\nSample of merged data:\")\n",
    "        display(merged_data[['country', 'year'] + list(new_cols)].head())\n",
    "    else:\n",
    "        print(\"No external data found, proceeding with enhanced data only\")\n",
    "        merged_data = enhanced_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee10c02",
   "metadata": {},
   "source": [
    "## 8. Complete Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RUNNING COMPLETE PROCESSING PIPELINE ===\")\n",
    "\n",
    "try:\n",
    "    # Run the complete processing pipeline\n",
    "    final_data = preprocessor.process_all_data()\n",
    "    \n",
    "    print(f\"‚úÖ Processing completed successfully!\")\n",
    "    print(f\"Final dataset shape: {final_data.shape}\")\n",
    "    \n",
    "    # Show final data summary\n",
    "    print(\"\\n=== FINAL DATA SUMMARY ===\")\n",
    "    print(f\"Total records: {len(final_data):,}\")\n",
    "    print(f\"Total columns: {len(final_data.columns)}\")\n",
    "    print(f\"Countries: {final_data['country'].nunique()}\")\n",
    "    print(f\"Years: {final_data['year'].nunique()}\")\n",
    "    print(f\"Year range: {final_data['year'].min()} - {final_data['year'].max()}\")\n",
    "    \n",
    "    # Missing values in final data\n",
    "    missing_final = final_data.isnull().sum().sum()\n",
    "    print(f\"Missing values: {missing_final} ({missing_final/final_data.size*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n=== COLUMN CATEGORIES ===\")\n",
    "    \n",
    "    # Categorize columns\n",
    "    id_cols = ['country', 'year']\n",
    "    prevalence_cols = [col for col in final_data.columns if 'prevalence' in col]\n",
    "    derived_cols = [col for col in final_data.columns \n",
    "                   if any(keyword in col for keyword in ['change', 'decade', 'since', 'total'])]\n",
    "    external_cols = [col for col in final_data.columns \n",
    "                    if any(keyword in col.lower() for keyword in ['population', 'gdp'])]\n",
    "    other_cols = [col for col in final_data.columns \n",
    "                 if col not in id_cols + prevalence_cols + derived_cols + external_cols]\n",
    "    \n",
    "    print(f\"Identifier columns ({len(id_cols)}): {id_cols}\")\n",
    "    print(f\"Prevalence metrics ({len(prevalence_cols)}): {prevalence_cols}\")\n",
    "    print(f\"Derived features ({len(derived_cols)}): {derived_cols}\")\n",
    "    print(f\"External data ({len(external_cols)}): {external_cols}\")\n",
    "    print(f\"Other columns ({len(other_cols)}): {other_cols}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Processing failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef7984",
   "metadata": {},
   "source": [
    "## 9. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'final_data' in locals():\n",
    "    print(\"=== DATA VALIDATION ===\")\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # 1. Check data types\n",
    "    print(\"1. Data Types Validation:\")\n",
    "    numeric_cols = final_data.select_dtypes(include=[np.number]).columns\n",
    "    object_cols = final_data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    print(f\"   ‚úì Numeric columns: {len(numeric_cols)}\")\n",
    "    print(f\"   ‚úì Text columns: {len(object_cols)}\")\n",
    "    \n",
    "    # 2. Check value ranges for prevalence\n",
    "    print(\"\\n2. Prevalence Values Validation:\")\n",
    "    prevalence_cols = [col for col in final_data.columns if 'prevalence' in col]\n",
    "    \n",
    "    for col in prevalence_cols:\n",
    "        values = final_data[col].dropna()\n",
    "        min_val, max_val = values.min(), values.max()\n",
    "        \n",
    "        # Check if values are reasonable (0-100% for prevalence)\n",
    "        valid_range = (min_val >= 0) and (max_val <= 100)\n",
    "        status = \"‚úì\" if valid_range else \"‚ö†Ô∏è\"\n",
    "        \n",
    "        print(f\"   {status} {col}: {min_val:.2f}% - {max_val:.2f}%\")\n",
    "    \n",
    "    # 3. Check for duplicates\n",
    "    print(\"\\n3. Duplicate Records Check:\")\n",
    "    if 'country' in final_data.columns and 'year' in final_data.columns:\n",
    "        duplicates = final_data.duplicated(subset=['country', 'year']).sum()\n",
    "        status = \"‚úì\" if duplicates == 0 else \"‚ö†Ô∏è\"\n",
    "        print(f\"   {status} Duplicate country-year combinations: {duplicates}\")\n",
    "    \n",
    "    # 4. Check temporal consistency\n",
    "    print(\"\\n4. Temporal Consistency:\")\n",
    "    if 'year' in final_data.columns:\n",
    "        year_range = final_data['year'].max() - final_data['year'].min()\n",
    "        print(f\"   ‚úì Year span: {year_range} years\")\n",
    "        \n",
    "        # Check for reasonable year values\n",
    "        reasonable_years = (final_data['year'] >= 1990) & (final_data['year'] <= 2025)\n",
    "        unreasonable_count = (~reasonable_years).sum()\n",
    "        status = \"‚úì\" if unreasonable_count == 0 else \"‚ö†Ô∏è\"\n",
    "        print(f\"   {status} Records with unreasonable years: {unreasonable_count}\")\n",
    "    \n",
    "    # 5. Geographic coverage\n",
    "    print(\"\\n5. Geographic Coverage:\")\n",
    "    if 'country' in final_data.columns:\n",
    "        total_countries = final_data['country'].nunique()\n",
    "        print(f\"   ‚úì Total countries: {total_countries}\")\n",
    "        \n",
    "        # Countries with most data\n",
    "        top_countries = final_data['country'].value_counts().head(5)\n",
    "        print(f\"   ‚úì Top countries by data points:\")\n",
    "        for country, count in top_countries.items():\n",
    "            print(f\"      ‚Ä¢ {country}: {count} records\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Data validation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a73dc4",
   "metadata": {},
   "source": [
    "## 10. Final Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e529a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'final_data' in locals():\n",
    "    print(\"=== FINAL PROCESSED DATA OVERVIEW ===\")\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"Dataset shape: {final_data.shape}\")\n",
    "    print(f\"Memory usage: {final_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(final_data.head())\n",
    "    \n",
    "    # Summary statistics for numeric columns\n",
    "    numeric_cols = final_data.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(\"\\nSummary statistics for numeric columns:\")\n",
    "        display(final_data[numeric_cols].describe())\n",
    "    \n",
    "    # Create a simple visualization\n",
    "    if 'country' in final_data.columns and 'year' in final_data.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Data coverage over time\n",
    "        plt.subplot(1, 2, 1)\n",
    "        year_counts = final_data['year'].value_counts().sort_index()\n",
    "        plt.plot(year_counts.index, year_counts.values, 'o-', linewidth=2, markersize=6)\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Number of Records')\n",
    "        plt.title('Data Coverage Over Time')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Top countries by data availability\n",
    "        plt.subplot(1, 2, 2)\n",
    "        top_countries = final_data['country'].value_counts().head(10)\n",
    "        plt.barh(range(len(top_countries)), top_countries.values)\n",
    "        plt.yticks(range(len(top_countries)), top_countries.index)\n",
    "        plt.xlabel('Number of Records')\n",
    "        plt.title('Top 10 Countries by Data Availability')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Data preprocessing completed successfully!\")\n",
    "    print(\"üìÅ Processed data saved to: ../data/processed/mental_health_processed.csv\")\n",
    "    print(\"üìã Data summary saved to: ../data/processed/data_summary.json\")\n",
    "    print(\"üîÑ Ready for analysis - proceed to notebook 03_time_series_analysis.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
